{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_bert_fine_tune_consumer_complaints.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofieneJ/classification_CES_project_sources/blob/master/keras_bert_fine_tune_consumer_complaints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR8sloTT66rC",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "This work is based on Jacob Zweig's work https://github.com/strongio/keras-bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc_fi9sGZlia",
        "colab_type": "text"
      },
      "source": [
        "##Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqb0_BK3Vs4T",
        "colab_type": "code",
        "outputId": "949301df-7e17-403e-d2c3-de43e90e2c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx4lCy3-UpjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from bert.tokenization import FullTokenizer\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K\n",
        "import keras\n",
        "import gensim\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeBsXWbv8LDr",
        "colab_type": "text"
      },
      "source": [
        "### mounting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnqHQgI_U3-W",
        "colab_type": "code",
        "outputId": "2bac37a2-ffe8-4f4a-ee11-6e6791448276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "\n",
        "#@markdown 1- complaints_data_dir is the main dataset dir.\n",
        "#@markdown under the complaints_data_dir there should be the raw csv file with two columns ['Product', 'Consumer complaint narrative']\n",
        "complaints_data_dir = root_dir + 'my_colab_storage/consumer_complaints_data/'\n",
        "\n",
        "#@markdown 2- The dir to train/test split csv files. This director should be under complaints_data_dir\n",
        "evaluation_data_dir = complaints_data_dir+'train_test_data/'\n",
        "tf.gfile.MakeDirs(evaluation_data_dir)\n",
        "\n",
        "#@markdown 3- bert_path is the path to the BERT pretrained model dir. It can be replaced by the tensorflow-hub url: https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
        "bert_path = root_dir +'my_colab_storage/pre-trained-models/tf_hub_bert_uncased_L-12_H-768_A-12/'\n",
        "#@markdown 4- trained_model_path is the path to the trained model output\n",
        "trained_model_path = root_dir+'my_colab_storage/bert_fine_tuning/keras/classification_model/KerasCustomerComplaintsModel.h5'\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = False #@param {type:\"boolean\"}\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(trained_model_path)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "#tf.gfile.MakeDirs(trained_model_path)\n",
        "print('***** Model output path: {} *****'.format(trained_model_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "***** Model output path: /content/gdrive/My Drive/my_colab_storage/bert_fine_tuning/keras/classification_model/KerasClassificationModel.h5 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJdGlLgXcJvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 256\n",
        "min_seq_length = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evjr4sJeZqiH",
        "colab_type": "text"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0qoWk7YWajW",
        "colab_type": "code",
        "outputId": "440d3794-a4ff-40ee-a786-196a8042a53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "LABEL_COLUMN = 'Product'\n",
        "DATA_COLUMN = 'Consumer complaint narrative'\n",
        "\n",
        "\n",
        "def filter_subsample_split_data():\n",
        "  # label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "  # label_list = []\n",
        "  raw_data_file = complaints_data_dir +'Consumer_Complaints_light.csv'\n",
        "  complains_df = pd.read_csv(filepath_or_buffer=raw_data_file, sep=',')\n",
        "  my_categories = ['Debt collection', 'Mortgage', 'Credit reporting', 'Student loan', 'Credit card', 'Bank account or service', \n",
        "  'Checking or savings account', 'Consumer Loan', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service']\n",
        "\n",
        "  my_categories = ['Debt collection', 'Mortgage', 'Credit reporting', 'Student loan', 'Credit card']\n",
        "\n",
        "  complains_df = complains_df.loc[complains_df[LABEL_COLUMN].isin(my_categories)].reset_index(drop=True)\n",
        "  print ('size after category filter ', complains_df.shape)\n",
        "  # print (complains_df.head())\n",
        "  count_cols = [LABEL_COLUMN, 'Count']\n",
        "  complains_by_group = complains_df.groupby(by=LABEL_COLUMN,axis=0).count().reset_index().rename(columns = {DATA_COLUMN:'Count'})\n",
        "  # print (type(complains_by_group))\n",
        "  print (complains_by_group.sort_values(by=\"Count\", ascending =False))\n",
        "\n",
        "\n",
        "  ########################### Sub-sample the dataset #########################################\n",
        "  index_reduced = np.arange(0,complains_df.shape[0])\n",
        "  np.random.shuffle(index_reduced)\n",
        "  kept_data_ratio = 0.1\n",
        "  index_reduced = index_reduced[0:int(complains_df.shape[0]*kept_data_ratio)]\n",
        "  complains_df = complains_df.loc[index_reduced]\n",
        "\n",
        "  print ('complaints distribution after subsampling')\n",
        "  count_cols = [LABEL_COLUMN, 'Count']\n",
        "  complains_by_group = complains_df.groupby(by=LABEL_COLUMN,axis=0).count().reset_index().rename(columns = {DATA_COLUMN:'Count'})\n",
        "  # print (type(complains_by_group))\n",
        "  print (complains_by_group.sort_values(by=\"Count\", ascending =False))\n",
        "  print ('kept categories are:', np.sort(complains_df[LABEL_COLUMN].unique()))\n",
        "\n",
        "  ########################### test train split the dataset #########################################\n",
        "  cats = complains_df[LABEL_COLUMN]\n",
        "  corpus = complains_df[DATA_COLUMN]\n",
        "  corpus_train, corpus_test, cat_train, cat_test = train_test_split(corpus, cats, test_size = 0.3, random_state=100)\n",
        "  \n",
        "  ########################### Dump train test datasets #######################################\n",
        "  train_test_data_dir = complaints_data_dir +'train_test_data/'\n",
        "  corpus_train.to_csv(path_or_buf= train_test_data_dir+'train_corpus.csv', index = False, header = True)\n",
        "  corpus_test.to_csv(path_or_buf= train_test_data_dir+'test_corpus.csv', index = False, header = True)\n",
        "  cat_train.to_csv(path_or_buf= train_test_data_dir+'train_cat.csv', index = False, header = True)\n",
        "  cat_test.to_csv(path_or_buf= train_test_data_dir+'test_cat.csv', index = False, header = True)\n",
        "  print (f'train subset size: {corpus_train.shape}')\n",
        "  print (f'test subset size: {corpus_test.shape}')\n",
        "  \n",
        "filter_subsample_split_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size after category filter  (212171, 2)\n",
            "            Product  Count\n",
            "2   Debt collection  86850\n",
            "3          Mortgage  53056\n",
            "1  Credit reporting  31588\n",
            "4      Student loan  21839\n",
            "0       Credit card  18838\n",
            "complaints distribution after subsampling\n",
            "            Product  Count\n",
            "2   Debt collection   8595\n",
            "3          Mortgage   5265\n",
            "1  Credit reporting   3219\n",
            "4      Student loan   2218\n",
            "0       Credit card   1920\n",
            "kept categories are: ['Credit card' 'Credit reporting' 'Debt collection' 'Mortgage'\n",
            " 'Student loan']\n",
            "train subset size: (14851,)\n",
            "test subset size: (6366,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbpPv9KmfRL",
        "colab_type": "code",
        "outputId": "3db75227-755c-4a87-dfc8-8a08f99f5fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "def preprocess_data():\n",
        "  \n",
        "  REFUSED_DOC_TAG = '<<<refused_document>>>'\n",
        "  def simple_preprocess_func (doc):\n",
        "    seq = gensim.utils.simple_preprocess(gensim.parsing.remove_stopwords(doc), min_len=2, max_len=25)\n",
        "    if len(seq)<min_seq_length or len(seq)>max_seq_length:\n",
        "      return REFUSED_DOC_TAG\n",
        "    else:\n",
        "      return ' '.join(seq)\n",
        "    \n",
        "  \n",
        "  train_test_data_dir = complaints_data_dir +'train_test_data/'\n",
        "  subsets = ('train','test')\n",
        "  dfs = []\n",
        "  for subset in subsets:\n",
        "    ############################# corpus preprocessing ######################################\n",
        "    corpus_path = train_test_data_dir+f'{subset}_corpus.csv'\n",
        "    corpus_ser = pd.read_csv(filepath_or_buffer=corpus_path, sep=',')[DATA_COLUMN]\n",
        "    print (f'{subset} corpus size before preprocessing is {corpus_ser.shape}')\n",
        "    corpus_ser = corpus_ser.apply(lambda txt : simple_preprocess_func(txt))\n",
        "\n",
        "    valid_index = (corpus_ser != REFUSED_DOC_TAG).tolist()\n",
        "    corpus_ser = corpus_ser[valid_index].reset_index(drop=True)\n",
        "    print (f'{subset} corpus size after preprocessing is {corpus_ser.shape}')\n",
        "    \n",
        "    ###################################### Some statictics #####################################\n",
        "    doc_lens = np.array([len(doc.split()) for doc in corpus_ser])\n",
        "    print (f'mean length is {np.mean(doc_lens)}')\n",
        "    print (f'max length is {np.max(doc_lens)}')\n",
        "\n",
        "    \n",
        "    ###################################### Named category conversion ##########################\n",
        "    cat_path = train_test_data_dir+ f'{subset}_cat.csv'\n",
        "    cats_ser = pd.read_csv(filepath_or_buffer=cat_path, sep=',')[LABEL_COLUMN]\n",
        "    class_dico = {i:classe for i,classe in enumerate(np.sort(cats_ser.unique()))}\n",
        "    print ('classes', class_dico)\n",
        "    inv_class_dico = {item[1]:item[0] for item in class_dico.items()}\n",
        "    cats_ser = cats_ser.apply(lambda x : inv_class_dico[x])\n",
        "    cats_ser = cats_ser[valid_index].reset_index(drop=True)\n",
        "    #preprocessed_cat_path = f'data\\\\{subset}_cat_reduced.csv'\n",
        "    #cats_ser.to_csv(path_or_buf=preprocessed_cat_path, index=False)\n",
        "\n",
        "    assert(cats_ser.shape[0]==corpus_ser.shape[0])\n",
        "    \n",
        "    ###################################### returning DataFrames ##########################\n",
        "    data_dic = {DATA_COLUMN: corpus_ser, LABEL_COLUMN:cats_ser}\n",
        "    subset_df = pd.DataFrame(data_dic)\n",
        "    dfs.append(subset_df)\n",
        "  \n",
        "  label_list = [i for i in np.sort(dfs[0][LABEL_COLUMN].unique())]\n",
        "  \n",
        "  return dfs[0], dfs[1], label_list\n",
        "\n",
        "train_df, test_df, label_list = preprocess_data()\n",
        "print ('the label list is:', label_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train corpus size before preprocessing is (14851,)\n",
            "train corpus size after preprocessing is (13386,)\n",
            "mean length is 79.45472882115644\n",
            "max length is 256\n",
            "classes {0: 'Credit card', 1: 'Credit reporting', 2: 'Debt collection', 3: 'Mortgage', 4: 'Student loan'}\n",
            "test corpus size before preprocessing is (6366,)\n",
            "test corpus size after preprocessing is (5715,)\n",
            "mean length is 78.87419072615923\n",
            "max length is 255\n",
            "classes {0: 'Credit card', 1: 'Credit reporting', 2: 'Debt collection', 3: 'Mortgage', 4: 'Student loan'}\n",
            "the label list is: [0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p6S9zWwX6ZF",
        "colab_type": "code",
        "outputId": "f0c45398-c341-4aa6-9649-8e0ca1bbfbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\n",
        "# Create datasets (Only take up to max_seq_length words for memory)\n",
        "train_text = train_df[DATA_COLUMN].tolist()\n",
        "train_text = np.array(train_text, dtype=str)[:, np.newaxis]\n",
        "train_label = train_df[LABEL_COLUMN].tolist()\n",
        "print (f' train text shape {train_text.shape}')\n",
        "\n",
        "\n",
        "test_text = test_df[DATA_COLUMN].tolist()\n",
        "test_text = np.array(test_text, dtype=str)[:, np.newaxis]\n",
        "test_label = test_df[LABEL_COLUMN].tolist()\n",
        "print (f' test text shape {test_text.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " train text shape (13386, 1)\n",
            " test text shape (5715, 1)\n",
            " train text shape (13386, 1)\n",
            " test text shape (5715, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCAaeiqdZslb",
        "colab_type": "text"
      },
      "source": [
        "##BERT formating and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhAl35i8ZChr",
        "colab_type": "code",
        "outputId": "6d51611b-2397-4513-9ab2-98da0a653cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, 1),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_text, train_label)\n",
        "test_examples = convert_text_to_examples(test_text, test_label)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d05cc49eb5a344bab4baffbc7c54ad89",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=13386, style=ProgressSt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf7db9feced6457b8e3f3ef4c9072841",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=5715, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc0Wd96EbIax",
        "colab_type": "text"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TEkjPqzaQo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"first\",\n",
        "        bert_path=bert_path,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "        if self.pooling not in [\"first\", \"mean\"]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "        if self.pooling == \"first\":\n",
        "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "            trainable_layers = [\"pooler/dense\"]\n",
        "\n",
        "        elif self.pooling == \"mean\":\n",
        "            trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "            trainable_layers = []\n",
        "        else:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        if self.pooling == \"first\":\n",
        "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"pooled_output\"\n",
        "            ]\n",
        "        elif self.pooling == \"mean\":\n",
        "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result, input_mask)\n",
        "        else:\n",
        "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztlMQFXXbG9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_output = BertLayer(n_fine_tune_layers=1, pooling=\"mean\")(bert_inputs) #pooling=\"first\"\n",
        "    dense = tf.keras.layers.Dense(units=512, activation='relu')(bert_output) #units=256\n",
        "    #dense2 = tf.keras.layers.Dense(units=256, activation='relu')(dense1) #units=256\n",
        "    pred = tf.keras.layers.Dense(units=len(label_list), activation='softmax')(dense) #sigmoid\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #binary_crossentropy\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0hVH2SpbSW9",
        "colab_type": "code",
        "outputId": "a6f25c8f-83aa-44d3-c5ef-9ab161b19111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model = build_model(max_seq_length)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "categorical_train_labels = keras.utils.to_categorical(train_labels, num_classes=len(label_list))\n",
        "categorical_test_labels = keras.utils.to_categorical(test_labels, num_classes=len(label_list))\n",
        "\n",
        "current_time = datetime.now()\n",
        "model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    categorical_train_labels,\n",
        "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], categorical_test_labels),\n",
        "    epochs=2,\n",
        "    batch_size=32\n",
        ")\n",
        "print(\"Training and evaluation took time \", datetime.now() - current_time)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            2565        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 110,501,183\n",
            "Trainable params: 7,484,165\n",
            "Non-trainable params: 103,017,018\n",
            "__________________________________________________________________________________________________\n",
            "Train on 13386 samples, validate on 5715 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/2\n",
            "13376/13386 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8171"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6fd821b68020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_segment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_test_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training and evaluation took time \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    410\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXnztvzVoWIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.save(trained_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj4KNkJn6dZl",
        "colab_type": "text"
      },
      "source": [
        "## Predictions performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yfd0Imhb7RH",
        "colab_type": "code",
        "outputId": "0176e241-b44c-45ed-ccd6-ada19567c20e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "source": [
        "model = build_model(max_seq_length)\n",
        "initialize_vars(sess)\n",
        "model.load_weights(trained_model_path)\n",
        "\n",
        "post_save_preds = model.predict([test_input_ids, \n",
        "                                test_input_masks, \n",
        "                                test_segment_ids]\n",
        "                            ) # predictions after we clear and reload model\n",
        "\n",
        "# all(pre_save_preds == post_save_preds) # Are they the same?\n",
        "print (post_save_preds.shape)\n",
        "predicted_classes = np.argmax(post_save_preds, axis=1)\n",
        "\n",
        "\n",
        "print (classification_report(test_labels,predicted_classes)) #,target_names = my_cats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_2 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          393728      bert_layer_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 5)            2565        dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,501,183\n",
            "Trainable params: 7,484,165\n",
            "Non-trainable params: 103,017,018\n",
            "__________________________________________________________________________________________________\n",
            "(5715, 5)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       557\n",
            "           1       0.87      0.76      0.81       864\n",
            "           2       0.89      0.90      0.90      2405\n",
            "           3       0.93      0.91      0.92      1308\n",
            "           4       0.81      0.92      0.86       581\n",
            "\n",
            "    accuracy                           0.88      5715\n",
            "   macro avg       0.86      0.87      0.86      5715\n",
            "weighted avg       0.88      0.88      0.88      5715\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioTivLbgExdJ",
        "colab_type": "text"
      },
      "source": [
        "## prediction on test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e3LGegvgHBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_single_sample_with_finetuned_model(model, tokenizer, sample):\n",
        "    seq = gensim.utils.simple_preprocess(gensim.parsing.remove_stopwords(sample), min_len=2, max_len=25)\n",
        "    if (len(seq) > max_seq_length):\n",
        "        raise NameError(\n",
        "                f\"sequence is too long: the obtained size after preprocessing is {len(seq)}. Maximum size is {max_seq_length}\"\n",
        "            )\n",
        "    if (len(seq)<min_seq_length):\n",
        "        raise NameError(\n",
        "                f\"sequence is too short: the obtained size after preprocessing is {len(seq)}. Minimum size is 10\"\n",
        "            )\n",
        "    \n",
        "    bert_example = convert_text_to_examples(texts = [seq], labels=[-1])\n",
        "    # bert_sample = InputExample(guid=None, text_a=\" \".join(seq), text_b=None, label=-1)\n",
        "    # Instantiate tokenizer\n",
        "    \n",
        "\n",
        "    input_id, input_mask, segment_id, _ = convert_single_example(\n",
        "        tokenizer, bert_example[0], max_seq_length\n",
        "    )\n",
        "\n",
        "    #model = build_model(max_seq_length)\n",
        "    #initialize_vars(sess)\n",
        "    #model.load_weights(trained_model_path)\n",
        "\n",
        "    post_save_preds = model.predict([[input_id], \n",
        "                                    [input_mask], \n",
        "                                    [segment_id]]\n",
        "                                ) # predictions after we clear and reload model\n",
        "\n",
        "    # all(pre_save_preds == post_save_preds) # Are they the same?\n",
        "    \n",
        "    return post_save_preds[0]\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTsxOyOuhRaH",
        "colab_type": "code",
        "outputId": "9789c011-4bbd-4ff5-8ac3-481e89f2f075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "dico= {0: 'Credit card', 1: 'Credit reporting', 2: 'Debt collection', 3: 'Mortgage', 4: 'Student loan'}\n",
        "inv_class_dico = {item[1]:item[0] for item in dico.items()}\n",
        "\n",
        "              \n",
        "evaluation_data_dir = complaints_data_dir+'test_corpus/'\n",
        "corpus_path = evaluation_data_dir+'test_corpus.csv'\n",
        "label_path = evaluation_data_dir+'test_cat.csv'\n",
        "\n",
        "example_ser = pd.read_csv(filepath_or_buffer=corpus_path, sep=',')[DATA_COLUMN]\n",
        "cat_ser = pd.read_csv(filepath_or_buffer=label_path, sep=',')[LABEL_COLUMN]\n",
        "\n",
        "evaluation_df = pd.DataFrame({DATA_COLUMN:example_ser, LABEL_COLUMN:cat_ser})\n",
        "#print (evaluation_df.head(2))\n",
        "\n",
        "\n",
        "###############loading model and tokenizer######################\"\"\n",
        "#tokenizer = create_tokenizer_from_hub_module()\n",
        "#model = build_model(max_seq_length)\n",
        "#initialize_vars(sess)\n",
        "#model.load_weights(trained_model_path)\n",
        "\n",
        "\n",
        "#############evaluation loop#############################\n",
        "for index, row in evaluation_df.iterrows():\n",
        "  example_text  = row[DATA_COLUMN]\n",
        "  example_cat  = row[LABEL_COLUMN]\n",
        "\n",
        "  \n",
        "  try:\n",
        "    liste_prob = predict_single_sample_with_finetuned_model (model, tokenizer, example_text)\n",
        "    #if (np.argmax(liste_prob)!=inv_class_dico[example_cat]):\n",
        "    print ('** Email text:', example_text)\n",
        "    print ('** Real category:', example_cat)\n",
        "    print ('** Predicted probabilities: ', dict(zip(inv_class_dico,liste_prob)))\n",
        "    print ('*********************************************NEXT EXAMPLE********************************')\n",
        "      \n",
        "  except NameError:\n",
        "    print ('preprocessing error')\n",
        "  \n",
        "  \n",
        "  if index==20:\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Email text: I was recently considered late on a payment ( payment was made just after XXXX on the due date ). The payment was made on the due date and the cut off is arbitrary for an online payment. I make all my payments online and Discover is the only company that imposes this. The second complaint is the amount of the late charge being excessive. The balance is {$1000.00} and my average monthly payment has been {$36.00}. The late charge alone was {$35.00} and doubled my most recent payment. Last, the 18 % interest rate is outrageous. I have good credit and my credit score is over XXXX. I can get a lower rate if I apply for a new card. I do n't want another card with Discover. I stopped using it when they changed a due without notification causing a late payment. They also sent the account to a collection agency even though it was not delinquent and never had been. It took months to be resolved. So NO WAY do I want to do business with Discover. \n",
            "\n",
            "What I want is the late charge reduced and the interest rate lowered.\n",
            "** Real category: Credit card\n",
            "** Predicted probabilities:  {'Credit card': 0.8757109, 'Credit reporting': 0.06247546, 'Debt collection': 0.048909586, 'Mortgage': 0.0066837636, 'Student loan': 0.006220219}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: This is not my debt. I attempted to contact original debtor ( XXXX XXXX ) but was informed that I needed to resolve this issue with the debt collection agency. I contacted the agency and was advised to complete a dispute.\n",
            "** Real category: Debt collection\n",
            "** Predicted probabilities:  {'Credit card': 0.00213491, 'Credit reporting': 0.015765563, 'Debt collection': 0.9809976, 'Mortgage': 0.00047509136, 'Student loan': 0.00062664587}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: On XX/XX/XXXX We sold a Ford F250 truck to an individual. We did not remove the license plate from the vehicle at the time of sale. The new owner apparently did not register the vehicle in his name and acquire a new license plate. He acquired XXXX parking violations ( XX/XX/XXXX and XXXX ) which totaled {$20.00} each plus penalties, for a total of {$160.00}. He failed to pay the tickets and we were sent the citations, because the license plates still showed us to be the owner. When the City of XXXX XXXX sent us the citation I sent them a letter and copy of bill of sale and title transfer. They continued to send notices and I would each time send copies. They referred the matter to XXXX XXXX XXXX and they began harassing calls and letters. I sent them the copies as well. XXXX XXXX has now passed on the debt to Transworld Systems who has continued the harassment. I copied them on the same information on XX/XX/XXXX. In response, On XX/XX/XXXX I received their \" validation, '' which is the same info of the date, time, ticket #, truck plate. I want all correspondence to cease regarding this debt which is clearly not ours. In addition to the collection agencies we also had a revocation of motor vehicle registration privileges placed on us by the XXXX DPS, which required more documentation and time to clear up.\n",
            "** Real category: Debt collection\n",
            "** Predicted probabilities:  {'Credit card': 0.0026139205, 'Credit reporting': 0.01575117, 'Debt collection': 0.9806378, 'Mortgage': 0.00020795628, 'Student loan': 0.00078920275}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: Every two months, Wells Fargo loses my online account info and locks me out. They also \" lose '' the contact info I call every two months to correct. And although they record calls, they hang up if I tell them I too am recording the call to protect myself. I called back and again informed them I am recording the call and the woman told me \" then I ca n't talk to you. '' When I asked for supervisor, she put me on hold. An hour ago. I am still holding. I am completely at their mercy!\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.10348693, 'Credit reporting': 0.013589326, 'Debt collection': 0.68427294, 'Mortgage': 0.07311088, 'Student loan': 0.1255399}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I reconsolidated my student loan debt thinking it was the smart thing to do. I work for a non-profit agency and have for the past 10 years. I reconsolidated back in XX/XX/XXXX. \n",
            "\n",
            "I was nit too familiar with the company I chose to refinance and was overwhelmed at the time due to financial difficulties of going through a divorce with a child. \n",
            "\n",
            "I pay my loan to XXXX XXXX of {$180.00}, in the middle of the month, but then I get a separate charge taken directly out of my bank account under Equitable Acceptance of {$45.00} at the end of the month. Equitable Acceptance is a third party, but I never seem to get a clear answer as to why I being charged this fee when the whole idea is for me to pay less and have loan forgiveness in 10 years.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.00042748128, 'Credit reporting': 0.0060455687, 'Debt collection': 0.033138726, 'Mortgage': 0.0014801287, 'Student loan': 0.95890814}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I tried to pay Navient by credit card today by phone. I did this last month. This time they told me that they were no longer accepting credit cards. They said that they informed me of this new change, but they did n't. They tried to get me to pay out of a checking account, but I do n't have it. Now, I have yo try to get a cash advance on my credit card st a much higher rate. That 's the only way I can get XXXX dollars in a day. I think they 're trying to force me to be late, so they can charge higher rates. They failed to correct my address four times in the past to do this.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.19639465, 'Credit reporting': 0.060384702, 'Debt collection': 0.40139592, 'Mortgage': 0.0019652594, 'Student loan': 0.3398595}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I received a loan modification from OCWEN and have made all the payments before the due date but they have assessed a late charge for every payment and have ignored my requests to have them removed even after I provided bank records. I assume they have reported the late payments to the credit bureaus.\n",
            "** Real category: Mortgage\n",
            "** Predicted probabilities:  {'Credit card': 0.053312432, 'Credit reporting': 0.08969583, 'Debt collection': 0.07821546, 'Mortgage': 0.38383678, 'Student loan': 0.39493948}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I had approximately {$26000.00} in loans from various companies when I finished school. I quickly decided to consolidate, I notified all of the companies. I had no problems until I discovered that everything was transferred to XXXX but around {$2500.00} was somehow left with NAVIENT. NAVIENT reported me as late. I have maintained a near perfect credit score all of my life but what was a in the 780s range is now in the 560s range. NAVIENT will not help my although I have mailed in every possible document, I have email them many time and spent hours on the phone. I have spoken with the Department of the Customer Advocate many times. All I get told is that there is nothing they can do. I have spent countless house researching what went wrong and I admit that I did make a few mistakes during the consolidation process but at this point I feel that NAVIENT is simply unwilling to review this mishap or take into consideration that I have never had a late or missed payment in my life which is why my credit score was almost perfect. \n",
            "\n",
            "NAVIENT can not send a letter detailing conversations and say they consider the matter closed and expect this to go away.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.0009886574, 'Credit reporting': 0.028590122, 'Debt collection': 0.051747255, 'Mortgage': 0.0012122678, 'Student loan': 0.9174617}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I filed a complaint XX/XX/XXXX after pulling my credit report and notice there were Hard Inquires that I had no business with. I called and even mailed letters to Experian telling them they are not mine and had no business with those companies. I want them removed from my credit report. \n",
            "\n",
            "I would like for all those Inquires to be removed at once. \n",
            "1 ) XXXX XXXX XXXX 2 ) XXXX XXXX 3 ) XXXX 4 ) XXXX 5 ) XXXX XXXX XXXX ) XXXX XXXX\n",
            "** Real category: Credit reporting\n",
            "** Predicted probabilities:  {'Credit card': 0.011143295, 'Credit reporting': 0.85126543, 'Debt collection': 0.12066026, 'Mortgage': 0.004025924, 'Student loan': 0.012905016}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: XXXX financial lost 2 of my payments made in XXXX XXXX which were made in agreement for a deferment to secure my loan. As a result of their negligence in losing my payments which made the account more delinquent than it actually was and subsequently caused them to repose my vehicle on around XXXX XXXX XXXX. I disputed the missing payments with them by sending official bank statements and the bank ( XXXX XXXX ) speaking to them verifying the payments were made and on what days. XXXX XXXX refused to accept my official bank statements, debit card transaction and payment receipts as proof of payment. XXXX also never sent me any documentation about the repossession or the auction within the 14 day time frame as required by law. I filed a dispute against XXXX XXXX concerning the documentation that I nor the primary borrower received with the XXXX. XXXX XXXX claims they sent the documentation but to an incorrect address despite the fact they had repossessed the vehicle from the very address that was on file. In fact XXXX is still reporting to the credit bureaus as if this account is open and active showing late payments from the month of XXXX XXXX up until now even though the status should reflect chargeoff +\n",
            "** Real category: Credit reporting\n",
            "** Predicted probabilities:  {'Credit card': 0.047816817, 'Credit reporting': 0.48027334, 'Debt collection': 0.12547086, 'Mortgage': 0.12903187, 'Student loan': 0.21740715}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "preprocessing error\n",
            "** Email text: 1. Consumer contacted on XXXX XXXX for a billing which was rendered less than XXXX days ago XXXX XXXX, 2015. 2. Customer paid half the billing without even allowed to review it. Address change and move to new address happened on XXXX XXXX, 2015. 3. Agency was given new address and consumer still has not received billing. Agency refused to identify its license number or forward calls to upper management or give email address for complaint and resolution. Credit Management Associates advised that the invoicing would be stayed for XXXX days and this has not happened as of XXXX, Full name of all management personnel refused access. Violation of and inappropriate contact for a {$68.00} with which {$34.00} was paid pending receipt of invoice. Collection Personnel not allowing access or refusing to take message is XXXX XXXX. Mgr who does not take calls are XXXX XXXX and president who does not answer or take calls is XXXX XXXX.\n",
            "** Real category: Debt collection\n",
            "** Predicted probabilities:  {'Credit card': 0.025942735, 'Credit reporting': 0.21255699, 'Debt collection': 0.73702747, 'Mortgage': 0.0056533567, 'Student loan': 0.018819397}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I signed up for the credit card offered through the website XXXX when I was looking for airfares and was approved. Sadly though my experience with the credit card has been awful. Trying to pay my credit card statement was awful. I have been using XXXX and XXXX XXXX cards for years and have never had a single issue with payments or connecting with banks for online payments. When I attempted to connect the bank account I would be using for payments the master card website showed the wellsfargo account as an option for payments. It appeared that I was able to connected XXXX XXXX checking account with the XXXX XXXX card online before a payment was due, I was never informed there was an issue, in fact once I attempted to make a payment online ( several days before a payment was due ) XXXX XXXX Card sent me an email to notify me that a payment was pending, then within a few days on XXXX XXXX 2016 I received another email from XXXX Card informing me that payment was received and applied to my credit card. Seemed like everything was ok until today I go online to look at my statement from XXXX XXXX Card and it says \" Your payment is past due '' and they have charged me over {$64.00} in interest. I then contacted XXXXCard and they informed me that they contacted the bank XXXX and they could not find the account. But XXXX Card made no attempt to inform me of this and I later contacted XXXX XXXX and their records show no contact from XXXX. I have been using credit card for years and have never had an issue like this and I have a good credit score, I pay my bills, I pay my bills on time. I do n't want my credit score to be affected by their indecent managment and inability to communicate with their clients effectively. And I also do n't want to pay the interest as I do n't think the interest was incurred by fault of myself. I feel like they are trying to take advantage of busy people by making a simple process complicated and they offered little to no help when I called XXXX Card, instead they blamed me that I provided the wrong information. I am sorry to say that once my balance is paid I will be canceling this card and most likely will never apply for a XXXX Card again.\n",
            "** Real category: Credit card\n",
            "** Predicted probabilities:  {'Credit card': 0.87762403, 'Credit reporting': 0.04958948, 'Debt collection': 0.059284214, 'Mortgage': 0.012385237, 'Student loan': 0.0011170438}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: Earnest ( www.earneset.com ) asked about monthly expenses, including rent. I do not pay rent ( my spouse pays all of the rent and since they do not consider spousal income, they should not consider spousal monthly expenses ). They refused to accept that I do not pay rent when considering my loan and offered me a higher interest rate than I should have qualified for based on my monthly expenses. Additionally, they refuse to consider assets and property when calculating interest rates. They are currently considering household debt and monthly expenses, but refuse to consider household income. I believe these tactics of selectively considering income and assets is in an effort to ensure they are able to keep their interest rates high and make their advertised interest rates unattainable. This occurred from XX/XX/XXXX-XX/XX/2018.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.030006861, 'Credit reporting': 0.096980765, 'Debt collection': 0.4397324, 'Mortgage': 0.2224668, 'Student loan': 0.21081324}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: Paid Loan off, have letter. Another company Mohela is asking me to pay off loan, again.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.0003347771, 'Credit reporting': 0.0010883869, 'Debt collection': 0.04084638, 'Mortgage': 0.0034059063, 'Student loan': 0.95432454}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: XXXX XXXX/Global Credit work together, at least at times. XXXX or all of these XXXX ( I believe XXXX XXXX ), submitted my name to the XXXX credit bureaus as a negative \" no payment ''. All XXXX credit bureaus now have a negative report on me. This is completely unfair and incorrect & below is why : Either someone has fraudulently opened a XXXX card in my name or XXXX XXXX/Global Credit has improperly done due diligence. About a year ago, a XXXX credit application was opened in my name. A small charge of {$38.00} was made. And for continual months thereafter - the XXXX bill was sent to another city with a different zip code! Thus, I never received a bill, and thus I never even knew a bill was supposedly due. Fast forward till now. I do a credit report and find out that my credit is tainted, and at all XXXX credit agencies. I call around & find that Global Credit has been in charge of collecting. But, they have NEVER called me, and I have never received a bill! I 've called ( beginning in XX/XX/XXXX ) Global credit and requested original credit application documentation, and continued calling them every XXXX weeks, and as of yesterday XX/XX/2015, they still do NOT have any supporting documentation of the credit card application. They have also confirmed to me that this XXXX bill was sent to a different city & zip code and NEVER to my address. Yet, in spite of never sending any bill to me, to pay, evaluate if mine, etc., they have gone right ahead & posted a negative credit rating XXXX all three credit agencies )! This is outrageous. You see, supposedly this account is registered in my name. But with a different city & zip code. Hence, I 've NEVER received a bill. Yet, they 've had the audacity to file a negative credit report against me - when this account is either a fraud account or has been mistakenly ( on their part ) mailed incorrectly to some other location and city. And the original bill was only {$38.00}! My credit history has been impeccably kept amazingly excellent over the years. I ALWAYS pay my bills and on time. But, I do n't pay for others ' bills, or for bills which I know nothing about, as I never receive a billing notice. Anyway, I 'll not quit till the XXXX credit reporting agencies have all cleared my credit history. And, I will demand from XXXX/XXXX/Global Credit a letter of apology, as they have done a huge mistake and error in judgement. Anything short of this, will result in legal recourse.\n",
            "** Real category: Debt collection\n",
            "** Predicted probabilities:  {'Credit card': 0.2173474, 'Credit reporting': 0.47882184, 'Debt collection': 0.29616496, 'Mortgage': 0.00552514, 'Student loan': 0.0021406782}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: I am filing this complaint because TransUnion has ignored my request to provide me with the documents that their company has on file that was used to verify the accounts I disputed. Being that they have gone past the 30 day mark and can not verify these accounts, under Section 611 ( 5 ) ( A ) of the FCRA - they are required to \" ... promptly delete all information which can not be verified '' that I have disputed. Please resolve this manner as soon as possible. Thank you.\n",
            "** Real category: Credit reporting\n",
            "** Predicted probabilities:  {'Credit card': 0.009119639, 'Credit reporting': 0.9585643, 'Debt collection': 0.024967145, 'Mortgage': 0.0031733615, 'Student loan': 0.004175482}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "preprocessing error\n",
            "** Email text: I am being contacted by a law firm for a debt I do not owe. I have kindly ask this debt collector to verify the information according to the laws of consumers. I have requested the original documents and verify the information accordingly. They have not been able to. I was a victim of identity theft in which I reported this information to the FTC and credit bureaus. This debt collector is harassing me for debt I do not owe and wants me to enter into an agreement with them that I am not required to do by law.\n",
            "** Real category: Debt collection\n",
            "** Predicted probabilities:  {'Credit card': 0.0016892339, 'Credit reporting': 0.0143906195, 'Debt collection': 0.9836653, 'Mortgage': 6.3383675e-05, 'Student loan': 0.00019138314}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: When checking my credit score it shows my amount increased by {$1300.00}. I contacted Navient to explain to what this increase is for. It seems everytime I am close to paying off my loan they add more to my loan. They can not explain to me what this amount is. Should I not have the right to know what the complete total is. That said is is money I owe to the school but that school is closed, so how do I still owe them money. Sounds very questionable and no one can give me a answer.\n",
            "** Real category: Student loan\n",
            "** Predicted probabilities:  {'Credit card': 0.00047441808, 'Credit reporting': 0.0068209046, 'Debt collection': 0.061811235, 'Mortgage': 0.00058064196, 'Student loan': 0.93031275}\n",
            "*********************************************NEXT EXAMPLE********************************\n",
            "** Email text: XXXX addresses are not mine : Address : XXXX XXXX XXXX, NY XXXXXXXXAddress identification number : XXXXType of Residence : Single familyGeographical Code : XXXXXXXXThe above address was where my ex-wife ran away with the man she had an affair with after our divorce. \n",
            "\n",
            "XXXX XXXX XXXX XXXX, XXXX, FL XXXX, is my employment business address, not a single family home\n",
            "** Real category: Credit reporting\n",
            "** Predicted probabilities:  {'Credit card': 0.019460637, 'Credit reporting': 0.49129128, 'Debt collection': 0.13762659, 'Mortgage': 0.2499651, 'Student loan': 0.10165638}\n",
            "*********************************************NEXT EXAMPLE********************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLnumwou_2UZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}